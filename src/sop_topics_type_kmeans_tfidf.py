import os
import json
from docx import Document
from io import StringIO, BytesIO
import re
import time
import datetime

import pandas as pd
import json
import spacy
from nltk.corpus import stopwords

from gensim.models import LdaModel
from gensim.models.wrappers import LdaMallet
import gensim.corpora as corpora
from gensim.corpora import Dictionary
from gensim import matutils, models
from gensim.models import CoherenceModel, TfidfModel, HdpModel
from gensim.models.phrases import Phrases, Phraser
import pyLDAvis.gensim

from sklearn.cluster import KMeans
from scipy.sparse import csc_matrix
from gensim.matutils import corpus2csc
from matplotlib.ticker import MaxNLocator
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity

from docx import Document
from io import StringIO, BytesIO


def load_event_role(sop_df, event_type, role):
    """
    Load json data organized by event types then
    generate data frame with event type, jurisdiction, role and sop instructions as columns

    Params
    ----
    sop_df: pd.DataFrame    read from 'data/interim/sop_types_valid.csv'
    event_type: str    Event Type (Call Type)
    role: str    specify the subject/role responsible for SOP instruction ('Call taker' or 'Dispatcher')

    Returns
    ----
    pd.DataFrame 
    """
    with open(F'data/sop_jsons/{event_type}.txt') as f:
        dct = json.load(f)
    f.close()
    event_row = sop_df[sop_df['type'] == event_type]
    juri_to_filename = dict(zip(event_row['juri'].values[0], 
                                event_row['filename'].values[0]))
    types, juris, roles, sops = list(), list(), list(), list()
    for juri, role_sop in dct.items():
        if role in role_sop:
            types.append(event_type)
            juris.append(juri)
            roles.append(role)
            sops.append(role_sop[role])
    df = pd.DataFrame({'type': types, 'juri': juris, 'role': roles, 'sop': sops})
    df['filename'] = df['juri'].apply(lambda x: juri_to_filename[x])
    return df




def load_event_types_for_role(sop_df, types, role):
    """
    Concat all the dataframes generated by load_event_role

    Params
    ----
    sop_df: pd.DataFrame    read from 'data/interim/sop_types_valid.csv'
    event_type: str    Event Type (Call Type)
    role: str    specify the subject/role responsible for SOP instruction ('Call taker' or 'Dispatcher')

    Returns
    ----
    pd.DataFrame 
    """
    res = pd.DataFrame()
    for t in types:
        res = res.append(load_event_role(sop_df, t, role))
    return res.reset_index(drop = True)




def preprocess(nlp, 
               strlist,
               min_token_len = 2,
               allowed_pos = ['ADV', 'ADJ', 'VERB', 'NOUN', 'PART', 'NUM', 'PROPN']): 
    """
    Pre-process texts and return lemmatized words

    params
    ----
    nlp: Spacy en_core_web_sm model
    strlist : list of strings
    min_token_len: minimum length of lammetized tokens
    allowed_pos: allowable part-of-speech

    returns
    ----
    string of lammetized tokens

    """
    removal = ['-', r'i\.e\.']
    res = list()
    not_stopword = {'call'}
    for string in strlist:
        text = re.sub(r"|".join(removal), ' ', string.lower())
        doc = nlp(text)
        res += [token.lemma_ for token in doc \
                if token.pos_ in allowed_pos \
                and (token.text in not_stopword or not token.is_stop) \
                and len(token.lemma_) > min_token_len
               ]
    
    return ' '.join(res)




def get_dct_dtmatrix(nlp, sops):
    """
    Convert texts into Gensim dictionary and doc term matrix

    params
    ----
    nlp: Spacy en_core_web_sm model
    sops: list of strings

    returns
    ----
    doc_term_matrix: array of (int, int) for term frequency
    corpus: string of tokens
    dictionary: Gensim word2vec dictionary

    """
    corpus = [sop.split() for sop in map(lambda x: preprocess(nlp, x), sops)]
#     phrases = Phrases(corpus, min_count = 1, threshold = 1)
#     bigram = Phraser(phrases)
#     corpus = bigram(corpus)
    dictionary = corpora.Dictionary(corpus)
    doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]
    return doc_term_matrix, corpus, dictionary




def bow2tfidf(doc_term_bow, corpus_tfidf):
    """
    Apply tfidf conversion

    params
    ----
    doc_term_bow: array of (int, int) for term frequency
    corpus_tfidf: tfidf model fitted with doc_term_bow above

    returns
    ----
    tfidf_mtx: array of (int, float) 

    """
    doc_term_tfidf = corpus_tfidf[doc_term_bow]
    scipy_tfidf = corpus2csc(doc_term_tfidf, num_terms = len(corpus_tfidf.idfs))
    tfidf_mtx = csc_matrix(scipy_tfidf).T.toarray()
    return tfidf_mtx




def save_df(df, name, prefix = 'data/interim/'):
    """
    Save dataframe to disk

    params
    ----
    df: dataframe
    name: name of the saved .csv
    prefix: saving path

    """
    filename = prefix + name
    df.to_csv(filename, index = False)




def main():
    """
    Executes all the scripts defined above
    """
    nlp = spacy.load("en_core_web_sm")
    sop_df = pd.read_csv(
        'data/interim/sop_types_valid.csv', 
        converters = {'juri': eval, 
                      'filename': eval})
    type_list = sop_df['type']

    try:
        calltaker_all = pd.read_csv(
            'data/interim/calltaker_all.csv', 
            converters = {'sop': eval}
        )
    except:
        calltaker_all = load_event_types_for_role(sop_df, type_list, 'call taker')

    save_df(calltaker_all, 'calltaker_all.csv')    
    doc_term_bow, corpus, dictionary = get_dct_dtmatrix(nlp, calltaker_all['sop'])
    tfidf_type = TfidfModel(doc_term_bow)
    tfidf_mtx = bow2tfidf(doc_term_bow, tfidf_type)

    km_alltype = KMeans(n_clusters = 87, random_state = 911).fit(tfidf_mtx)
    type_topics_kmeans_tfidf = calltaker_all.copy()
    type_topics_kmeans_tfidf['cluster'] = km_alltype.labels_
    type_topics_kmeans_tfidf = type_topics_kmeans_tfidf.sort_values(
        by = ['cluster', 'type', 'juri'], ignore_index = True)
    type_topics_kmeans_tfidf.to_csv('data/interim/type_topics_kmeans_tfidf.csv', index = False)
    



if __name__ == "__main__":
    main()
